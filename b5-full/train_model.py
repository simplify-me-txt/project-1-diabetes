import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report
import joblib

print("ðŸš€ Starting Multi-Model Training...")
print("=" * 60)

# Load dataset (PIMA Indian Diabetes Dataset)
df = pd.read_csv("diabetes.csv")
print(f"âœ… Dataset loaded: {df.shape[0]} samples, {df.shape[1]} features")

X = df.drop("Outcome", axis=1)
y = df["Outcome"]

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

print(f"\nðŸ“Š Training set: {X_train.shape[0]} samples")
print(f"ðŸ“Š Test set: {X_test.shape[0]} samples")
print("\n" + "=" * 60)

# === MODEL 1: Logistic Regression ===
print("\nðŸ”¹ Training Model 1: Logistic Regression")
model_lr = LogisticRegression(max_iter=500, random_state=42)
model_lr.fit(X_train, y_train)
y_pred_lr = model_lr.predict(X_test)
accuracy_lr = accuracy_score(y_test, y_pred_lr)
print(f"   âœ“ Accuracy: {accuracy_lr:.4f} ({accuracy_lr*100:.2f}%)")

# === MODEL 2: Random Forest ===
print("\nðŸ”¹ Training Model 2: Random Forest")
model_rf = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    random_state=42,
    n_jobs=-1
)
model_rf.fit(X_train, y_train)
y_pred_rf = model_rf.predict(X_test)
accuracy_rf = accuracy_score(y_test, y_pred_rf)
print(f"   âœ“ Accuracy: {accuracy_rf:.4f} ({accuracy_rf*100:.2f}%)")

# === MODEL 3: XGBoost ===
print("\nðŸ”¹ Training Model 3: XGBoost")
model_xgb = XGBClassifier(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    random_state=42,
    eval_metric='logloss'
)
model_xgb.fit(X_train, y_train)
y_pred_xgb = model_xgb.predict(X_test)
accuracy_xgb = accuracy_score(y_test, y_pred_xgb)
print(f"   âœ“ Accuracy: {accuracy_xgb:.4f} ({accuracy_xgb*100:.2f}%)")

# === ENSEMBLE VOTING ===
print("\n" + "=" * 60)
print("ðŸŽ¯ Ensemble Voting System")
print("=" * 60)

# Majority voting
y_pred_ensemble = []
for i in range(len(y_test)):
    votes = [y_pred_lr[i], y_pred_rf[i], y_pred_xgb[i]]
    prediction = 1 if sum(votes) >= 2 else 0
    y_pred_ensemble.append(prediction)

accuracy_ensemble = accuracy_score(y_test, y_pred_ensemble)
print(f"\nðŸ“ˆ Ensemble Accuracy: {accuracy_ensemble:.4f} ({accuracy_ensemble*100:.2f}%)")

# Save all models and scaler
print("\n" + "=" * 60)
print("ðŸ’¾ Saving models...")
joblib.dump(model_lr, "diabetes_model_lr.pkl")
print("   âœ“ Logistic Regression saved")
joblib.dump(model_rf, "diabetes_model_rf.pkl")
print("   âœ“ Random Forest saved")
joblib.dump(model_xgb, "diabetes_model_xgb.pkl")
print("   âœ“ XGBoost saved")
joblib.dump(scaler, "scaler.pkl")
print("   âœ“ Scaler saved")

# Also save the old model for backward compatibility
joblib.dump(model_lr, "diabetes_model.pkl")
print("   âœ“ Legacy model saved")

print("\n" + "=" * 60)
print("âœ… Multi-Model AI System Training Complete!")
print("=" * 60)
print("\nðŸ“Š Model Performance Summary:")
print(f"   â€¢ Logistic Regression: {accuracy_lr*100:.2f}%")
print(f"   â€¢ Random Forest:       {accuracy_rf*100:.2f}%")
print(f"   â€¢ XGBoost:             {accuracy_xgb*100:.2f}%")
print(f"   â€¢ Ensemble (Voting):   {accuracy_ensemble*100:.2f}%")
print("\nðŸŽ‰ All models ready for production!\n")
